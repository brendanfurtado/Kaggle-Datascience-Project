{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Bernoulli(object):\n",
    "    def wj(self, X, y, i):\n",
    "        features = len(X.getnnz(axis=0))\n",
    "        X = X.toarray()\n",
    "        X = (X > 0).astype(np.int_)\n",
    "        y = [1 if element == i else 0 for element in y]\n",
    "        y = np.array(y)\n",
    "        wj1 = []\n",
    "        wj0 = []\n",
    "        for j in range(features):\n",
    "                theta_10 = (X[y==0][j].sum()+1) / (float((y==0).sum())+2)\n",
    "                theta_11 = (X[y==1][j].sum()+1) / (float((y==1).sum())+2)\n",
    "                wj1.append(np.log((theta_11)/(theta_10)))\n",
    "                wj0.append(np.log((1-theta_11)/(1-theta_10)))\n",
    "        return wj1, wj0\n",
    "\n",
    "    def prob_y1(self,y, i):\n",
    "        y = [1 if element == i else 0 for element in y]\n",
    "        y = np.asarray(y)\n",
    "        return y[y==1].sum() / float(len(y))\n",
    "\n",
    "    def prob_y0(self, y, i):\n",
    "        return 1-prob_y1(y, i)\n",
    "\n",
    "    def fit(self, X, y, X_test):\n",
    "        deci = []\n",
    "        for i in range(1,21):\n",
    "            save = []\n",
    "            wj1, wj0 = self.wj(X, y, i)\n",
    "            p1 = prob_y1(y, i)\n",
    "            p0 = prob_y0(y, i)\n",
    "            w0 = np.log(p1/p0) + np.sum(wj0)\n",
    "            w = np.subtract(wj1, wj0)\n",
    "            for j in range(len(X_test.getnnz(axis=1))):\n",
    "                wc = float(w*X_test[j].T)\n",
    "                save.append(float(wc + w0))\n",
    "            deci.append(np.asarray(save))\n",
    "        return deci\n",
    "\n",
    "    def predict(self,X, y, X_test):\n",
    "        deci = self.fit(X, y, X_test)\n",
    "        pred = []\n",
    "        for i in range(len(X_test.getnnz(axis=1))):\n",
    "            prob = []\n",
    "            for j in range(20):\n",
    "                prob.append(deci[j][i])\n",
    "            maxProb = max(prob)\n",
    "            for j in range(20):\n",
    "                if deci[j][i] == maxProb:\n",
    "                    pred.append(j+1)\n",
    "        pred = np.array(pred)\n",
    "        return pred\n",
    "    \n",
    "    import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "reddit_data = pd.read_csv(\"reddit_train.csv\") #train data\n",
    "df = pd.DataFrame(reddit_data) #train data\n",
    "reddit_data_copy = reddit_data.copy() #train data\n",
    "reddit_data_copy = reddit_data_copy.drop(columns=[\"subreddits\"]) #train data\n",
    "X = reddit_data_copy[reddit_data_copy.columns[1]]  #raw comments\n",
    "\n",
    "train_comments_list = []\n",
    "for i in range(len(X.index)):\n",
    "    train_comments_list.append(X.iloc[i])\n",
    "train_comments_list = list(train_comments_list)\n",
    "train_comments_list = np.array(train_comments_list)\n",
    "\n",
    "\n",
    "Y = reddit_data[reddit_data.columns[-1]]\n",
    "y_modified = []         #raw classes\n",
    "train_subreddits_class = []    #important, class in integer\n",
    "for i in range(len(Y.index)):\n",
    "    y_modified.append(Y.loc[i])\n",
    "for i in range(len(y_modified)):\n",
    "    if y_modified[i] == 'hockey':\n",
    "        train_subreddits_class.append(1)\n",
    "    elif y_modified[i] == 'nba':\n",
    "        train_subreddits_class.append(2)\n",
    "    elif y_modified[i] == 'leagueoflegends':\n",
    "        train_subreddits_class.append(3)\n",
    "    elif y_modified[i] == 'soccer':\n",
    "        train_subreddits_class.append(4)\n",
    "    elif y_modified[i] == 'funny':\n",
    "        train_subreddits_class.append(5)\n",
    "    elif y_modified[i] == 'movies':\n",
    "        train_subreddits_class.append(6)\n",
    "    elif y_modified[i] == 'Overwatch':\n",
    "        train_subreddits_class.append(7)\n",
    "    elif y_modified[i] == 'anime':\n",
    "        train_subreddits_class.append(8)\n",
    "    elif y_modified[i] == 'trees':\n",
    "        train_subreddits_class.append(9)\n",
    "    elif y_modified[i] == 'GlobalOffensive':\n",
    "        train_subreddits_class.append(10)\n",
    "    elif y_modified[i] == 'nfl':\n",
    "        train_subreddits_class.append(11)\n",
    "    elif y_modified[i] == 'AskReddit':\n",
    "        train_subreddits_class.append(12)\n",
    "    elif y_modified[i] == 'gameofthrones':\n",
    "        train_subreddits_class.append(13)\n",
    "    elif y_modified[i] == 'conspiracy':\n",
    "        train_subreddits_class.append(14)\n",
    "    elif y_modified[i] == 'worldnews':\n",
    "        train_subreddits_class.append(15)\n",
    "    elif y_modified[i] == 'wow':\n",
    "        train_subreddits_class.append(16)\n",
    "    elif y_modified[i] == 'europe':\n",
    "        train_subreddits_class.append(17)\n",
    "    elif y_modified[i] == 'canada':\n",
    "        train_subreddits_class.append(18)\n",
    "    elif y_modified[i] == 'Music':\n",
    "        train_subreddits_class.append(19)\n",
    "    elif y_modified[i] == 'baseball':\n",
    "        train_subreddits_class.append(20)\n",
    "train_subreddits_class = np.array(train_subreddits_class)\n",
    "X_train = train_comments_list\n",
    "y_train = train_subreddits_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yoooooooh/Desktop/Anaconda3/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\"] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "#preprocessing \n",
    "lem = WordNetLemmatizer()\n",
    "tknzr = WordPunctTokenizer()\n",
    "stem = PorterStemmer()\n",
    "replace_by_space = re.compile('[/(){}\\[\\]\\|@,;!.#><?\"\"''%$^*`:]')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "unused_char = re.compile('[^0-9a-zA-Z]')\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = X_train[i].lower()\n",
    "    X_train[i] = replace_by_space.sub(' ', X_train[i])\n",
    "    X_train[i] = unused_char.sub(' ', X_train[i])\n",
    "    X_train[i] = ' '.join(lem.lemmatize(word) for word in X_train[i].split() if word not in stop_words)\n",
    "\n",
    "tf_idf_vectorizer = CountVectorizer(stop_words = set(stopwords.words('english')), max_df = 1.0, ngram_range=(1, 3), min_df = 0.000005, analyzer='word',binary=True, tokenizer = WordPunctTokenizer().tokenize)\n",
    "vectors_train_idf = tf_idf_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression, mutual_info_classif\n",
    "skb = SelectKBest(chi2, k=10000)\n",
    "vectors_train_idf_normalized_selected = skb.fit_transform(vectors_train_nomalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 10000) (63000,)\n",
      "(7000, 10000) (7000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors_train_idf_normalized_selected, y_train, train_size=0.9, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 10000) (63000,)\n",
      "(7000, 10000) (7000,)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "cl = Bernoulli()\n",
    "pred = cl.predict(X_train, y_train, X_test)\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
